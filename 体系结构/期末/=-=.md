# 流水线部分

*   **为什么流水线的时钟周期 > (单周期CPU时钟周期 / 流水段个数)？** `[S04]`
    1.  **流水段间的寄存器延迟** `[S04]`：每个阶段之间需要插入流水线寄存器来保存中间结果。这些寄存器的读写本身就需要时间（延迟），构成了额外的开销 (Overhead)。
    2.  **流水段时长不均** `[S04]`：流水线的时钟周期必须迁就最慢的那个阶段。就像木桶原理，最短的板决定了容量。
    3.  **额外逻辑延迟** `[S04]`：为了处理流水线中的各种复杂情况（比如后面会讲到的“冒险”），需要增加如前递旁路等额外电路，这些也会增加延迟。

*   **为什么流水线的CPI > 1？** `[S05]`
    *   **CPI (Cycles Per Instruction)**：平均每条指令花费的时钟周期数。理想流水线中，每个周期都能完成一条指令，CPI = 1。
    *   **现实**：`[S05]` `实际CPI = 1 + Stall惩罚`。
    *   **Stall（停顿）** `[S05]`：为了保证正确性，流水线有时不得不暂停，就像装配线因为缺少零件而停工。这些停顿就是`Stall`。
    *   **Hazard（冒险/危害）** `[S05]`：导致停顿的“罪魁祸首”就是冒险。冒险是指由于流水线的并行特性，可能导致指令执行结果与串行执行不一致的潜在问题。主要分为数据冒险和控制冒险。

## 相关性
-
    1.  **数据相关性 (Data Dependence)**：
        *   **“先写，后读”（RAW, Read-After-Write）** `[S08]`：这也被叫做 **“真相关”**。后面一条指令要用前面一条指令刚写进去的数据。**这个是最核心的依赖，没办法取消，只能乖乖等前面那个写完了再读。**
            *   **例子**：
                `mul r2, r0, r1`  （这条指令用 r0 和 r1 的值，计算结果存到 r2 里）
                `add r4, r2, r3`  （这条指令要用 r2 的值，也就是上面 `mul` 指令算出来的值）
                看，`add` 指令就得等 `mul` 指令把结果写进 `r2` 之后才能读，*因为他是真正依赖于前一条指令的结果值*。
-
        *   **“先写，后写”（WAW, Write-After-Write）** `[S08]`：两条指令都要往同一个“地方”写东西。`[S09]` **“伪相关”**。
            *   **例子**：
                `mul r2, r0, r1`  （这条指令把结果写到 r2）
                `add r2, r1, r3`  （这条指令也把结果写到 r2）
                你看，`mul` 和 `add` 都想往 `r2` 里写。*但是实际上，add并不关心mul想写什么，他只是需要一个寄存器来存自己的结果。*
-
        *   **“先读，后写”（WAR, Write-After-Read）** `[S08]`：后面一条指令要往一个“地方”写数据，但前面一条指令还在用这个“地方”里的数据（在读）。`[S09]` **这也是一种“伪相关”**。
            *   **例子**：
                `mul r2, r1, r0`  （这条指令用了 r1 的值，算完结果存到 r2）
                `add r1, r3, r4`  （这条指令要往 r1 里写新值）
                这里，`add` 指令想往 `r1` 写，但 `mul` 指令（在他之前）还没读完 `r1` 的值呢！*同样的，add也是只是需要一个地方来存自己的计算结果。*
-
    2.  **控制相关性 (Control Dependence)** `[S07]`：流水线是“盲目自信”的，默认会把下一条指令吸进来。但如果遇到 if/else，CPU 在算出判断结果之前，根本不知道下一条该执行谁。这就叫控制相关。
	    1. 假如我们有汇编代码：
		```
		/* 指令 1 (分支指令) */
		BEQ r1, zero, TARGET   // 如果 r1 等于 0，就跳转到 TARGET 处
		/* 指令 2 (顺序执行) */
		ADD r2, r3, r4         // ！！！流水线会自动把这条吸进来
		/* 指令 3 */
		SUB r5, r6, r7
		...
		/* 目标位置 */
		TARGET:
		XOR r8, r9, r10        // 如果跳转成功，应该执行这条
		```
		2. 指令1在执行的时候才计算出来应该进行跳转，但是此时，流水线已经自动把指令2和3都吸附进来，此时就只能冲刷掉指令2和3，浪费了时间。
    3.  **结构相关性 (Structural Dependence)** `[S07]`：有两条指令，它们**同时都想用同一个硬件部件**。
		1. 假如我们有汇编代码：
		```
		/* 指令 1 (加载指令) */
		LW r1, 100(r2)     // Load Word: 需要去内存读数据
		/* 指令 2 */
		ADD r3, r4, r5
		/* 指令 3 */
		SUB r6, r7, r8
		/* 指令 4 */
		AND r9, r10, r11   // ！！！这一步需要去内存读指令代码
		```
		2. 在第四个时钟周期时：
			1. **指令 1 (LW)**：走了 4 步，刚好走到 **MEM (访存)** 阶段。它伸出手，要通过**内存接口**去读数据。
			2. **指令 4 (AND)**：刚准备进流水线，处于 **IF (取指)** 阶段。它伸出手，要通过**内存接口**去读指令本身的代码。
		3.  **后果**：不可能同时做。通常 **指令 1 优先**，**指令 4 被迫暂停 (Stall)** 一个周期，等指令 1 用完了再说。
		4. **总结**：`LW` 和 `AND` 没有任何数据上的关系，纯粹是因为它们在同一时刻都要用**内存硬件**，这就是**结构相关**。


# Cache部分

## 地址映射
### 1. 直接映射 (Direct Mapping) —— “一个萝卜一个坑”

*   **规则**：内存中的每一个块，只能放在 Cache 中**唯一特定**的位置。
* **地址划分**：`Tag | Index | Offset`
*   **优点**：
    *   **速度最快**：CPU 找数据时不需要犹豫，直接去那个固定的位置看一眼，“在就在，不在就不在”。
    *   **成本最低**：电路设计最简单。
*   **缺点**：
    *   **冲突高**：容易发生“抖动”（Thrashing）。如果程序频繁交替使用 15 号和 25 号数据，他俩就会不停地争抢 5 号座位，导致谁都坐不稳，效率极低。即使其他 9 个座位都是空的，他们也不能去坐。

---

### 2. 全相联映射 (Fully Associative Mapping) —— “随便坐”

*   **规则**：内存中的任何一个块，可以放在 Cache 中的**任何**一个位置。
*   **优点**：
    *   **利用率最高**：只要 Cache 没满，就不会发生冲突。只有当 10 个座位全满了，才需要踢人。
*   **缺点**：
    *   **查找最慢**：当 CPU 要找 15 号学生时，它不知道 15 号在哪，必须**同时检查所有 10 个座位**的标签（Tag）才能找到。
    *   **成本最高**：需要大量的比较器电路来实现“同时检查所有位置”，硬件昂贵且耗电。

---

### 3. 组相联映射 (Set Associative Mapping) —— “折中方案”

这是现代 CPU 最常用的方案。它是前两者的结合。

*   **规则**：
    1.  先把 Cache 分成若干个**“组” (Set)**。
    2.  内存块通过取模运算，确定它属于哪个**“组”**。
    3.  在这个 **“组”内**，它可以随便坐（类似全相联）。
* ***地址划分**：`Tag | Set Index | Offset`
*   **优点**：
    *   综合了前两者的长处。既不像直接映射那样容易冲突（因为组内有多个位置可选），也不像全相联那样查找困难（只需要在特定的组内搜索即可）。

## 替换策略

/
    2.  **🕰️ FIFO (先进先出 - First-In, First-Out)**
        *   **怎么做**：**谁最先来的，谁先滚蛋。** 就像超市货架的牛奶，最先摆上去的先拿走。
        *   **评价**：实现简单。但在计算机里有个大坑——**“老资格”不代表“没用了”**。有些核心代码虽然加载得很早，但每秒钟都要用，FIFO 会错误地把这种“老当益壮”的数据踢走，导致性能变差。
/
    3.  **📉 LRU (最近最少使用 - Least Recently Used)** `[S28]`
        *   **怎么做**：**踢掉那个“冷落”最久的数据。**
        *   **原理**：利用了**时间局部性**（刚才用过的，马上还会用；很久没用的，以后大概率不用）。
        *   **评价**：性能通常是**最好**的。但是！**硬件实现非常贵**。因为Cache必须时刻拿个小本本记着几百个块谁是第一名、谁是倒数第一，对于高关联度（格子多）的Cache，记录成本太高。

## Cache参数-命中率
![[Pasted image 20260101154310.png]]


|                                 |                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                         |                                                                          |
| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| 优化技术                            | 对 Miss (去图书馆次数) 的影响                                                                                                                                                                                                                                                                                                | 对 Latency (耗时) 的影响                                                                                                                                                                                                                                                      | 修正后的核心逻辑 (对应图表)                                                          |
| **1. 增大 Cache Size**<br>(换个大书桌) | <ul><li>**Compulsory**: 〓 不变</li><li>**Capacity**: 📉 **减少**<br><span style="color:gray;font-size:0.9em">（桌子大，能装！）</span></li><li>**Conflict**: 📉 **减少**<br><span style="color:gray;font-size:0.9em">（空间大，不挤！）</span></li></ul>                                                                                   | <ul><li>**Hit Latency**: 📈 **增加**<br><span style="color:gray;font-size:0.9em">（桌子越大，扫视一圈越慢）</span></li><li>**Miss Penalty**: 〓 不变</li></ul>                                                                                                                            | **大桌子**<br>能装东西，但这导致找东西稍微变慢一点点。                                          |
| **2. 增加关联度**<br>(允许乱放书)         | <ul><li>**Compulsory**: 〓 不变</li><li>**Capacity**: 〓 不变</li><li>**Conflict**: 📉 **减少**<br><span style="color:gray;font-size:0.9em">（核心作用：不打架）</span></li></ul>                                                                                                                                                    | <ul><li>**Hit Latency**: 📈 **增加**<br><span style="color:gray;font-size:0.9em">（位置不固定，检查起来费脑子）</span></li><li>**Miss Penalty**: 〓 不变</li></ul>                                                                                                                          | **灵活停放**<br>解决了抢位置冲突，但管理员查车比较慢。                                          |
| **3. 增加 Block Size**<br>(一次搬大箱) | <ul><li>**Compulsory**: 📉 **减少**<br><span style="color:gray;font-size:0.9em">（**核心作用**：顺手牵羊，一次拿一堆）</span></li><li>**Capacity**: 📉 **减少** <br><span style="color:red;font-size:0.9em"></span><br><span style="color:gray;font-size:0.9em">（一次搬得多，有效利用率高了，相当于容量“变大”了）</span></li><li>**Conflict**: ❓ 不确定</li></ul> | <ul><li>**Hit Latency**: 〓 **不变** <br><span style="color:red;font-size:0.9em"></span><br><span style="color:gray;font-size:0.9em">（箱子大不影响我看标签的速度）</span></li><li>**Miss Penalty**: 📈 **增加**<br><span style="color:gray;font-size:0.9em">（箱子越重，路上搬运越慢）</span></li></ul> | **批发进货**<br>强制性Miss和容量Miss都少了（进货效率高），找书速度也没变，唯一代价就是路上搬运（Miss Penalty）慢了。 |


## 写策略

### 第一组：命中（Write Hit）时的策略

当 CPU 要修改的数据已经存在于 Cache 中时，有两种处理方式：

#### 1. 写穿透 (Write-through) —— 也叫“写直达”
*   **动作**：CPU 在更新 Cache 中数据的同时，**立刻**也把数据写入主存（Memory）。
*   **特点**：
    *   **数据一致性高**：Cache 和主存的数据始终保持一致。
    *   **速度慢**：因为每次写操作都要访问速度较慢的主存，性能受限于主存带宽。
    *   **实现简单**：不需要额外的标记位。

#### 2. 写返回 (Write-back) —— 也叫“写回”
*   **动作**：CPU **只更新 Cache** 中的数据，**不**立刻写入主存。
    *   这个被修改过的 Cache 块会被标记为“脏”（Dirty）。
    *   只有当这个“脏”块需要被替换出去（比如Cache满了，要腾位置）时，才会把它写回到主存中。
*   **特点**：
    *   **速度快**：因为不需要频繁访问主存，写操作基本以 Cache 的速度进行。
    *   **数据不一致**：在一段时间内，Cache 中的数据是新的，主存中的数据是旧的。
    *   **实现复杂**：需要增加“Dirty bit”（脏位）来记录哪个块被修改过。

---

### 第二组：未命中（Write Miss）时的策略

当 CPU 要修改的数据**不在** Cache 中（Cache Miss）时，有两种处理方式：

#### 3. 写分配 (Write-allocate)
*   **动作**：先把要写的数据所在的块从主存**调入 Cache**，然后在 Cache 中进行修改。
*   **逻辑**：基于局部性原理，系统认为“如果你现在写了这个数据，你马上可能又要读它或写它”，所以先把它搬进 Cache 比较划算。
*   **常见搭配**：通常与 **写返回 (Write-back)** 搭配使用。

#### 4. 写不分配 (No-write-allocate)
*   **动作**：直接把数据写入主存，**不**将该块调入 Cache。
*   **逻辑**：系统认为“你只是写一下，后面可能不会再用到了”，所以不要占用宝贵的 Cache 空间。
*   **常见搭配**：通常与 **写穿透 (Write-through)** 搭配使用。

---

### 总结与常见组合

在现代计算机系统中，这四个策略通常是成对出现的：

| 组合方式 | 策略名称 | 解释 | 优缺点 |
| :--- | :--- | :--- | :--- |
| **组合 A (主流)** | **写返回 + 写分配**<br>(Write-back + Write-allocate) | 命中时只改Cache；<br>未命中时把数据拉进Cache再改。 | **性能最好**。利用了局部性原理，尽量减少主存访问。现代高性能 CPU（如 Intel Core, AMD Ryzen）的 L1/L2 Cache 多用此策略。 |
| **组合 B** | **写穿透 + 写不分配**<br>(Write-through + No-write-allocate) | 命中时改Cache并同步内存；<br>未命中时直接改内存，不管Cache。 | **最简单、最可靠**。虽然慢，但保证了数据绝对一致。常用于对数据一致性要求极高的场景（如显存某些部分）或低成本控制器。 |

# TLB页表

## 虚拟内存的作用


#### **知识卡片：虚拟内存的好处 `[S42]`**

*   **1. 程序隔离 / 保护** `[S42]`：
    *   每个进程都在自己的“沙箱”（私有虚拟地址空间）里运行，无法直接访问其他进程的物理内存。`[S42]` 这大大提高了系统的稳定性和安全性。
    *   `[S42]` 每个页还可以设置**读/写/执行权限**，由硬件强制保证。例如，代码段可以设为只读+可执行，防止被意外修改。

*   **2. 高效的内存使用**：
    *   **内存共享** `[S42]`：可以将同一个物理页映射到多个进程的虚拟地址空间，实现高效的进程间通信或共享库（如 C 语言库）。
    *   **按需分页 (Demand Paging)**：程序启动时，无需将整个程序都加载到内存。只需加载必要的部分，其他部分留在磁盘上。当访问到不在内存中的页时，会触发一个**缺页异常 (Page Fault)**，由操作系统负责从磁盘加载。这使得我们可以运行比物理内存更大的程序。

*   **3. 简化内存管理**：
    *   对程序员来说，他们面对的是一个简洁、连续的巨大内存空间，无需关心物理内存的碎片化问题。
    *   对操作系统来说，它可以自由地将物理页分配给任何虚拟页，管理起来更灵活。


## 虚拟地址与物理地址转换


#### **知识卡片：地址转换机制 `[S43]`**

*   **要解决的问题**：
    CPU 产生一个虚拟地址，如何一步步得到对应的物理地址？

*   **地址的拆分** `[S43]`：
    一个虚拟地址 (VA) 被分为两部分：
    `VA = [ 虚拟页号 (VPN) | 页内偏移 (Page Offset) ]`

    一个物理地址 (PA) 也被分为两部分：
    `PA = [ 物理页号 (PPN) | 页内偏移 (Page Offset) ]`

    *   **页内偏移 (Page Offset)** `[S43]`：`[S43]` 指明在一个页内部的字节位置。**地址转换过程中，页内偏移是不变的**。如果页大小为 $2^k$ 字节，那么偏移就占 `k` 位。
    *   **虚拟页号 (VPN)** `[S43]`：虚拟地址空间中的页编号。
    *   **物理页号 (PPN)** `[S43]`：物理内存中的页框编号。

    **地址转换的核心任务** `[S43]`：**就是将 VPN 转换为 PPN**。

*   **示例** `[S43]`：
    *   页大小 64KB = $2^{16}$ B -> 页内偏移占 16 位。
    *   32-bit 计算机 -> 虚拟地址 32 位 -> VPN 占 32 - 16 = 16 位。
    *   最大 256MB 物理内存 = $2^{28}$ B -> 物理地址 28 位 -> PPN 占 28 - 16 = 12 位。

*   **页表 (Page Table) 的作用** `[S44]`：
    *   `[S44]` 页表是一个**数组**，其**索引**是 **VPN**。
    *   存储在 `PageTable[VPN]` 处的内容，就是**页表项 (Page Table Entry, PTE)**。
    *   PTE 中最重要的信息就是 **PPN**。此外，还包含有效位、权限位、脏位等控制位。

*   **地址翻译流程** `[S44]`：
    1.  从虚拟地址中提取 VPN。
    2.  使用 VPN 作为索引，在当前进程的页表中查找对应的 PTE。
    3.  从 PTE 中提取 PPN。
    4.  将 PPN 和原始的页内偏移拼接起来，形成最终的物理地址。

## 页表结构与TLB工作原理

### 页表结构
页表是存储在**内存（RAM）** 中的数据结构，用于记录 VPN 到 PFN 的映射关系。

#### 1. 线性页表 / 单级页表 (Linear/Single-level Page Table)

- **原理**：一个大的数组，数组索引是VPN，值是PTE（Page Table Entry，页表项）。
    
- **优点**：实现简单，索引快。
    
- **缺点**：**内存占用过大**。例如32位系统，4KB页大小，需要100万个PTE。如果每个PTE 4字节，页表就需要4MB连续内存。对于64位系统，单级页表的大小是无法接受的。
    
- **关键点**：需要连续的物理内存存放页表。

#### 2. 多级页表 (Multi-level / Hierarchical Page Table)
（了解即可
- **原理**：将页表“分页”。使用树状结构，如页目录（Page Directory）指向二级页表，二级页表指向物理页。x86-64通常使用4级页表。
    
- **优点**：
    
    - **节省空间**：如果某段虚拟地址未被分配，对应的二级页表无需创建（按需分配）。
        
    - **离散存储**：页表本身可以不占用连续的大块物理内存。
        
- **缺点**：**时间换空间**。一次地址转换需要访问多次内存（4级页表需要4次内存访问才能拿到物理地址），延迟高。

掌握页表结构、映射方式、页表大小的含义及增减页表的影响，多级页表了解即可。[^1]

### TLB

*   **要解决的问题（直觉）**：
    `[S50]` 无论是单级还是多级页表，地址转换都需要访问内存（查页表）。这意味着，原本一次访存操作（比如 `load` 指令），现在变成了 **“查 L1 页表 -> 查 L2 页表 -> ... -> 访问真正的数据”**，访存次数翻倍甚至更多，性能损失是毁灭性的。

*   **解决方案：TLB** `[S51]`
    *   **形式化表述（严格）** `[S51]`：
        **TLB (Translation Lookaside Buffer)** 是一个**专门用于缓存近期用过的虚拟页号 (VPN) 到物理页号 (PPN) 映射关系**的小型、高速硬件缓存。它本质上是**页表的一个 Cache**。

    *   **工作流程**：
        1.  CPU 产生一个虚拟地址。
        2.  硬件**首先**用 VPN 并行地在 TLB 中查找。
        3.  **TLB Hit (命中)**：太好了！直接从 TLB 中获得 PPN，组合成物理地址，然后访问 Cache/内存。整个过程非常快（通常在一个时钟周期内）。
        4.  **TLB Miss (未命中)**：`[S50]` TLB 里没有这个翻译。此时，硬件或软件（OS）需要去**遍历页表 (Page Table Walk)**，找到对应的 PTE，`[S53]` 将其加载到 TLB 中，然后重新进行地址翻译。

    *   **TLB 的特性** `[S51]`：
        *   **容量小**：通常只有 16-512 个条目。
        *   **关联度高**：通常是全相联或高路组相联，以减少冲突。
        *   利用的是**页表访问的时间局部性**。

#### **知识卡片：TLB Miss vs. Page Fault `[S53]` `[S54]`**

#### 1. TLB Miss (小纸条上没记，但书在架子上)

##### 1. 硬件管理 TLB (Hardware-managed TLB)

这是目前主流通用处理器（Desktop/Server/Mobile）采用的方式。

*   **代表架构**：**x86/x86-64** (Intel, AMD), **ARM** (Cortex-A系列), PowerPC。
*   **核心机制**：**硬件页表遍历器 (Hardware Page Table Walker)**
    *   CPU 内部集成了专门的硬件单元（状态机或微码），称为 Page Table Walker。
    *   当发生 TLB Miss 时，CPU **不会**触发异常，也不会切换到操作系统内核态。
    *   硬件自动暂停流水线，根据控制寄存器（如 x86 的 `CR3` 或 ARM 的 `TTBR`）中保存的页表基地址，按照预定义的层级结构访问物理内存，找到对应的页表项（PTE）。
    *   硬件自动将 PTE 填入 TLB，然后重启刚才暂停的访存指令。
*   **优点**：
    *   **高性能**：省去了上下文切换（Context Switch）、保存寄存器、陷入内核（Trap）以及刷新指令流水线的巨大开销。处理延迟通常较短。
    *   **软件透明**：操作系统只需在初始化时建立好符合硬件规范的页表，后续的 TLB 维护对程序员和 OS 几乎是透明的。
*   **缺点**：
    *   **结构僵化**：页表的结构（如几级页表、每级大小、PTE 格式）被固化在硬件电路中，操作系统无法灵活改变页表算法（只能遵循硬件规范，如必须使用基数树 Radix Tree）。
    *   **硬件复杂度高**：增加了芯片设计的复杂度。

##### 2. 软件管理 TLB (Software-managed TLB)

这主要常见于精简指令集（RISC）的早期设计或嵌入式领域，强调硬件设计的极简主义。

*   **代表架构**：**MIPS**, **SPARC**, **Alpha**, 以及部分嵌入式处理器。
*   **核心机制**：**TLB 缺失异常 (TLB Miss Exception)**
    *   CPU 内部没有专门遍历页表的硬件电路。
    *   当发生 TLB Miss 时，CPU 无法处理，直接触发一个**异常（Exception/Trap）**。
    *   CPU 状态从用户态切换到**内核态**，程序计数器（PC）跳转到预设的 **TLB Miss 处理程序（Handler）** 入口。
    *   **操作系统**接管控制权，通过软件代码查询内存中的页表（计算 VPN 对应的 PTE 地址）。
    *   OS 使用特权指令（如 MIPS 的 `tlbwi` - TLB Write Index）手动将 PTE 写入 TLB。
    *   OS 执行从异常返回指令（如 `eret`），CPU 重新执行刚才那条访存指令。
*   **优点**：
    *   **极高的灵活性**：操作系统可以自由定义页表结构。可以使用多级页表、反向页表（Inverted Page Table）、哈希表甚至链表，硬件互不干涉。
    *   **硬件精简**：节省了片上晶体管资源，降低了 CPU 设计难度和功耗。
*   **缺点**：
    *   **性能开销大**：触发异常会导致流水线冲刷（Pipeline Flush），且需要保存/恢复上下文，相比硬件处理，时钟周期开销显著增加（通常慢 10~100 倍）。

---

### 总结对比表

| 特性 | 硬件管理 (Hardware-managed) | 软件管理 (Software-managed) |
| :--- | :--- | :--- |
| **处理主体** | MMU 中的硬件单元 (Page Walker) | 操作系统的异常处理程序 (OS Handler) |
| **TLB Miss 响应** | 硬件停顿，自动查询，无异常触发 | **触发 CPU 异常**，陷入内核态 |
| **页表结构** | **固定** (必须符合硬件规范，如 x86 四级页表) | **灵活** (OS 可自定义，如哈希表、树) |
| **速度/延迟** | 快 (仅内存访问延迟) | 慢 (包含异常处理、上下文切换开销) |
| **典型架构** | x86, ARM, PowerPC | MIPS, SPARC, Alpha |
| **设计哲学** | 性能优先，软硬耦合 | 硬件极简，软硬解耦 |

---

#### 2. 缺页异常 (Page Faults) - (小纸条没有，书架上也没有)

这种情况叫“缺页”。这是个**大麻烦**。

*   **这是什么意思？**
    你要找书，**“小纸条” (TLB)** 上没有，你去查 **“总索引目录” (页表)**，发现目录上写着：“本书当前不在书架上”。
    *   **情况A（Segmentation Fault）：** 这书压根就不存在，你记错名字了。-> **直接报错崩溃**（这就是为什么编程经常遇到段错误）。
    *   **情况B（True Page Fault）：** 书是有的，但是书架放不下了，被搬到 **“地下仓库” (硬盘)** 去了。

    这时候**必须操作系统**亲自出马，流程很繁琐：

    1.  **腾地儿：** 书架满了，得选一本现在的书把它**踢走**（页面置换算法）。
    2.  **写回（如果需要）：** 如果被踢走的那本书被乱涂乱画过（Dirty Page，“脏”页），得先把修改的内容抄回地下仓库保存好。如果没改过，直接扔了就行。
    3.  **去仓库取书：** 去地下仓库（硬盘）把你要的那本书搬上来。
        *   *注意：* 这个过程**巨慢无比！** (约 10ms)。对CPU来说，这漫长得像过了一个世纪。
        *   所以，在搬书的时候，CPU 会转头去处理别的任务（调度其他进程），不干等着。
    4.  **登记：** 书搬上架了，更新“总索引目录”和“小纸条”。
    5.  **重试：** 假装无事发生，重新执行刚才那个找书的动作。

## LRU和FIFO替换策略

应该和Cache部分差不多。

## 页大小对系统性能的影响

*   **关于页大小的思考** `[S49]`：
    *   **大页 (Large Pages) 的优缺点** `[S49]`：
        *   **优点**：
            *   **减少页表空间**：页变大，页的数量就变少，PTE 数量也变少。
            *   **减少页表层数/查找更快**：VPN 位数减少，可能让多级页表的层数也减少。
        *   **缺点**：
            *   **页内空间浪费（内部碎片）更严重**：`[S49]` 比如程序只需要 5KB 空间，但系统必须分配一个 2MB 的大页给它，造成了巨大的浪费。
            *   实现更复杂。


# 乱序执行

## 托马斯（Tomasulo）算法


#### **知识卡片：Tomasulo 算法 `[S64]`**

*   **核心思想**：
    Tomasulo 算法是 IBM System/360 Model 91 的浮点运算单元中首次提出的一种经典的动态调度算法，它完美地实现了上述乱序执行的四个条件。

*   **两大核心机制** `[S64]`：
    1.  **分布式保留站 (Reservation Station)**：`[S64]` 每个计算单元（如加法器、乘法器）前都有一组自己的保留站。指令根据其操作类型被分发到相应的保留站队列中。
    2.  **通过寄存器重命名消除伪相关 (WAW & WAR)** `[S64]`：
        *   这是 Tomasulo 算法的精髓所在。它通过将指令的目标寄存器“重命名”为将要产生该结果的**保留站的 ID (tag)**，从而彻底消除了 WAR 和 WAW 这种因为寄存器名不够用而产生的伪相关，只保留了真正的 RAW 数据依赖。

*   **关键组件（参照图示）** `[S66]`：
* ![[Pasted image 20260101205621.png]]
    *   **保留站 (Reservation Station)**：
        *   `Op`: 操作码。
        *   `Vj, Vk`: 操作数的值。如果已就绪，就存放在这里。
        *   `Qj, Qk`: 操作数的来源 (tag)。如果未就绪，这里存放的是将要产生该操作数的保留站的 ID。0 表示已就绪。
    *   **寄存器结果状态 (Register Result Status)**：
        *   可以看作是寄存器文件的扩展。`[S66]` 每个寄存器除了有 `value` 字段，还有一个 `st` (status/tag) 字段。
        *   如果 `st` 为空，表示寄存器的值是最新且可用的。
        *   如果 `st` 不为空，表示最新的值将由 ID 为 `st` 的保留站计算出来，任何需要该寄存器的指令都必须等待这个 tag。
    *   **公共数据总线 (Common Data Bus, CDB)** `[S66]`：
        *   连接所有执行单元的输出、所有保留站的输入以及寄存器文件的输入。
        *   `[S65]` 当计算完成时，结果（值 + tag）通过 CDB 广播给所有需要它的单元。

---

### **动手实践：逐步分解 Tomasulo 算法示例 `[S67-S73]`**

让我们一步步追踪一组指令在 Tomasulo 架构中的执行过程，来亲身体验它的魔力。

![[Pasted image 20260101210436.png]]

**初始状态** `[S67]`：
*   **指令序列**:
    1.  `div f0, f1, f2`
    2.  `mul f3, f0, f2`
    3.  `add f0, f1, f2`
    4.  `mul f3, f0, f2`
*   **寄存器**: `f1`=1.0, `f2`=1.0。其他寄存器的 `st` 字段为空。
*   **保留站**: 所有保留站都为空。

**第 1 步: 发射 `div f0, f1, f2`** `[S68]`
*   **动作**: `div` 指令被发射到 Mul/Div 保留站的一个空位（我们称之为 `c`）。
*   **状态变化**:
    *   **RS `c`**: `Op`=div, `Vj`=1.0 (来自`f1`), `Vk`=1.0 (来自`f2`), `Qj`=0, `Qk`=0。指令已就绪，开始执行（除法很慢）。
    *   **寄存器 `f0`**: `st` 字段被更新为 `c`。这表示 `f0` 的新值将由 `c` 站提供。
* ![[Pasted image 20260101210642.png]]

**第 2 步: 发射 `mul f3, f0, f2`** `[S69]`
*   **动作**: `mul` 指令被发射到 Mul/Div 保留站的另一个空位（称之为 `b`）。
![[Pasted image 20260101210805.png]]

*   **状态变化**:
    *   **RS `b`**: `Op`=mul。
        *   查询源操作数 `f0`：发现 `st[f0]` 是 `c`，表示 `f0` 的值还没好。于是 `Qj` 字段被设为 `c`。
        *   查询源操作数 `f2`：`st[f2]` 为空，值可用。于是 `Vk`=1.0，`Qk`=0。
    *   **寄存器 `f3`**: `st` 字段被更新为 `b`。

**第 3 步: 发射 `add f0, f1, f2`** `[S70]`
![[Pasted image 20260101213334.png]]
*   **动作**: `add` 指令被发射到 Add/Sub 保留站的一个空位（称之为 `z`）。
*   **状态变化**:
    *   **RS `z`**: `Op`=add, `Vj`=1.0 (来自`f1`), `Vk`=1.0 (来自`f2`), `Qj`=0, `Qk`=0。指令已就绪，可以立即执行！
    *   **寄存器 `f0`**: `st` 字段被更新为 `z`。
        *   **关键点 (WAW 消除)**: 注意！现在 `f0` 的“未来值”提供者从 `c` 变成了 `z`。这意味着第一条 `div` 指令的结果实际上已经被后续指令“忽略”了（对于 `f0` 而言）。

**第 4 步: 发射 `mul f3, f0, f2`** `[S71]`
![[Pasted image 20260101214142.png]]
*   **动作**: 第二条 `mul` 指令被发射到 Mul/Div 保留站的空位（称之为 `a`）。
*   **状态变化**:
    *   **RS `a`**: `Op`=mul。
        *   查询源操作数 `f0`：此时 `st[f0]` 是 `z`。于是 `Qj` 字段被设为 `z`。
        *   查询源操作数 `f2`：值可用。于是 `Vk`=1.0，`Qk`=0。
    *   **寄存器 `f3`**: `st` 字段被更新为 `a`。

**第 5 步: `add` 指令写回** `[S72]`
![[Pasted image 20260101214315.png]]

*   假设 `add` 指令（在 `z` 站）先执行完，结果为 `1.0 + 1.0 = 2.0`。
*   **动作**: 在 CDB 上广播结果 `<tag=z, value=2.0>`。
*   **状态变化**:
    *   **寄存器 `f0`**: 监听到 `tag=z`，与自己的 `st` 字段匹配。于是更新 `value[f0]`=2.0，并清空 `st[f0]`。
    *   **RS `a`**: 监听到 `tag=z`，与自己的 `Qj` 字段匹配。于是更新 `Vj`=2.0，并清空 `Qj`。现在 `a` 站的两个操作数都已就绪，可以执行了！
    *   **RS `z`**: 任务完成，被清空。

**第 6 步: `div` 指令写回** `[S73]`
![[Pasted image 20260101214419.png]]

*   很久以后，`div` 指令（在 `c` 站）执行完了，结果为 `1.0 / 1.0 = 1.0`。
*   **动作**: 在 CDB 上广播结果 `<tag=c, value=1.0>`。
*   **状态变化**:
    *   **寄存器 `f0`**: `st[f0]` 此时为空，与 `c` 不匹配。**无操作**。`div` 对 `f0` 的写入被自然地忽略了。
    *   **RS `b`**: 监听到 `tag=c`，与自己的 `Qj` 字段匹配。于是更新 `Vj`=1.0，并清空 `Qj`。现在 `b` 站也可以执行了。
    *   **RS `c`**: 任务完成，被清空。

- 核心思想：了解以冯诺依曼算法为例的乱序执行所需硬件结构、工作原理及各部分作用，无需了解详细过程。[^3]
    
- 关键概念：掌握托马斯算法的核心思想，如计算单元维护保留站的作用，通过寄存器重命名消除相关性；理解精确异常和 reorder buffer 的概念，以及它们与保留站的区别。[^2]

# 分支预测

## 2-bit饱和计数器

#### **知识卡片：动态分支预测 - 2-bit 饱和计数器 `[S76]` `[S77]` `[S78]`**

*   **机制** `[S77]`：
    *   使用 2 个 bit 来表示 4 个状态，为每个分支维护一个这样的计数器。
    *   **状态定义** `[S78]`：
        *   **11**: 强跳转 (Strongly Taken, T)
        *   **10**: 弱跳转 (Weakly Taken, t)
        *   **01**: 弱不跳转 (Weakly Not Taken, n)
        *   **00**: 强不跳转 (Strongly Not Taken, N)
    *   **预测规则** `[S77]`：最高位决定预测结果。1x -> 预测跳转；0x -> 预测不跳转。
    *   **更新规则（饱和计数）** `[S78]`：
        *   **实际跳转 (T)**：计数器 **+1** (除非已经是11，饱和了)。
        *   **实际不跳转 (N)**：计数器 **-1** (除非已经是00，饱和了)。

*   **内联小图（2-bit 预测器状态机）**：
    ```mermaid
    graph TD
        S11("预测: T (11)")
        S10("预测: T (10)")
        S01("预测: N (01)")
        S00("预测: N (00)")

        S00 --"实际 T"--> S01
        S01 --"实际 T"--> S10
        S10 --"实际 T"--> S11
        S11 --"实际 T"--> S11

        S11 --"实际 N"--> S10
        S10 --"实际 N"--> S01
        S01 --"实际 N"--> S00
        S00 --"实际 N"--> S00

        style S11 fill:#a7c7e7
        style S10 fill:#a7c7e7
        style S01 fill:#f6a6b2
        style S00 fill:#f6a6b2
    ```
    *   **[Fig·S78-1]** 2-bit 饱和计数器状态转换图。`[S78]` 清晰地展示了四个状态以及它们如何根据实际的分支结果进行转换。从一个“强”状态（11或00）需要连续两次相反的结果才能翻转到另一个预测区。

*   **性能分析 (回到循环例子)** `[S79]`：
    *   `j<3` 的行为序列：**T, T, T, N**...
    *   **稳定后**: 计数器会稳定在“强跳转”(11)状态。
        *   **第1次 T**: 状态11, 预测 T, 实际 T -> 猜对。
        *   **第2次 T**: 状态11, 预测 T, 实际 T -> 猜对。
        *   **第3次 T**: 状态11, 预测 T, 实际 T -> 猜对。
        *   **第4次 N**: 状态11, 预测 T, 实际 N -> **猜错**。状态变为 10 (弱跳转)。
    *   **结论** `[S79]`：稳定之后，每次内层循环**只有一次预测错误**！准确率提高到 75%。
    *   **优缺点** `[S79]`：
        *   `+` 预测准确率显著提高（通常能达到 85-90%）。
        *   `-` 硬件开销更大，需要为大量分支维护一个模式历史表 (Pattern History Table)，用指令地址的低位来索引这个表。 `[S77]`

## GHR和PHT

在现代CPU的**分支预测（Branch Prediction）** 机制中，**GHR (Global History Register)** 和 **PHT (Pattern History Table)** 是两个最核心的概念。它们通常协同工作，构成了**两级自适应分支预测器（Two-Level Adaptive Branch Predictor）** 的基础。

简单来说：
*   **GHR** 负责记录“**过去发生了什么**”（历史路径）。
*   **PHT** 负责记录“**在该历史下，通常会发生什么**”（概率/规律）。

下面详细介绍这两个组件及其工作原理。

---

### 1. GHR：全局历史寄存器 (Global History Register)

#### 定义
GHR 是一个位移寄存器（Shift Register），用于记录 CPU 最近执行的 $N$ 个分支指令的跳转结果。

#### 工作原理
*   **编码：** 通常用 `1` 代表分支跳转（Taken），用 `0` 代表分支不跳转（Not Taken）。
*   **移位更新：** 每当遇到一条分支指令并得知其结果后，GHR 会左移一位，并将最新的结果写入最低位（LSB）。旧的历史被移出。
*   **“全局”的含义：** 它是**全局**的，意味着它记录的是程序执行流中**所有**最近遇到的分支指令的混合历史，而不区分具体是哪一条指令（例如 PC=0x100 的分支和 PC=0x200 的分支结果都会进同一个 GHR）。

#### 作用
GHR 的主要目的是捕捉**分支之间的相关性（Correlation）**。
例如代码逻辑：
```c
if (a > 0) { ... } // 分支 A
if (b > 0) { ... } // 分支 B
if (a > 0 && b > 0) { ... } // 分支 C
```
如果分支 A 跳转（a>0）且分支 B 跳转（b>0），那么分支 C 极大概率也会跳转。GHR 通过记录 A 和 B 的结果（例如 `11`），为预测 C 提供上下文线索。

---

### 2. PHT：模式历史表 (Pattern History Table)

#### 定义
PHT 是一个存储器阵列（也就是一张表），表中的每一个条目（Entry）通常是一个**饱和计数器（Saturating Counter）**。

#### 结构
*   **大小：** PHT 包含 $2^M$ 个条目，其中 $M$ 通常与 GHR 的位数相关。
*   **内容：** 每个条目存储的是一个状态值，用来表示在特定的历史模式下，下一次分支更有可能跳转还是不跳转。
*   **2位饱和计数器（最常用）：**
    大多数 PHT 使用 2-bit 计数器，状态如下：
    *   `11`：Strongly Taken（强跳转）
    *   `10`：Weakly Taken（弱跳转）
    *   `01`：Weakly Not Taken（弱不跳转）
    *   `00`：Strongly Not Taken（强不跳转）

#### 作用
PHT 充当了“数据库”的角色。它记住了在过去遇到某种特定的 GHR 历史模式时，分支指令最终是跳转了还是没跳转。通过查表，预测器可以根据“经验”做出判断。

---

### 3. 两者如何协同工作？(工作流程)

这是理解分支预测核心逻辑的关键。最经典的实现是 **GShare** 或 **GSelect** 机制。

#### 第一步：索引生成 (Indexing)
CPU 需要在 PHT 表中找到一个条目来进行预测。如何找到这个条目？需要一个**索引（Index）**。
索引通常由 **GHR** 和 **分支指令的地址（PC）** 共同生成。
*   **简单法：** 直接用 GHR 的值作为 PHT 的索引。
*   **GShare（常用）：** 将 GHR 的值与 分支指令 PC 的部分位进行 **异或（XOR）** 运算。
    *   *为什么要 XOR？* 为了让不同的分支指令在拥有相同历史（GHR）时，能映射到 PHT 的不同位置，减少冲突（Aliasing）。

#### 第二步：查找与预测 (Lookup & Prediction)
1.  利用计算出的索引，从 PHT 中读取对应的 2-bit 计数器值。
2.  **做出预测：**
    *   如果计数器的高位是 `1`（即状态为 11 或 10），预测为 **Taken**。
    *   如果计数器的高位是 `0`（即状态为 01 或 00），预测为 **Not Taken**。

#### 第三步：更新 (Update)
当分支指令在流水线后端实际执行完毕，得知真实结果后，需要更新 PHT 和 GHR：
1.  **更新 GHR：** 将真实结果（0或1）移入 GHR。
2.  **更新 PHT：** 找到刚才使用的那个 PHT 条目，根据真实结果调整计数器：
    *   如果真实结果是 Taken，计数器加 1（直到饱和为 11）。
    *   如果真实结果是 Not Taken，计数器减 1（直到饱和为 00）。

---

### 4. 举例说明

假设 GHR 长度为 4 位，PHT 有 16 个条目。
当前 **GHR = 1101** (表示最近4次分支结果为：跳、跳、不跳、跳)。
当前遇到一条分支指令，假设经过运算，定位到 PHT 的第 13 号条目（Index = 1101）。

1.  **查表：** 查看 PHT[13] 的值。
2.  **情况 A：** 如果 PHT[13] = `10` (Weakly Taken)。
    *   **预测：** CPU 预测该分支会 **跳转 (Taken)**。
3.  **真实执行：**
    *   假设代码逻辑导致该分支真的跳转了。
4.  **更新：**
    *   **PHT 更新：** PHT[13] 从 `10` 变为 `11` (Strongly Taken)。下次再遇到这个模式，预测信心更强。
    *   **GHR 更新：** 左移一位，写入 1。GHR 变为 `1011`。

---

# 多发射
超标量和超长指令字
![[Pasted image 20260102142319.png]]

# 多线程

/
    *   **细粒度多线程 (Fine-Grained Multithreading)** `[S85]`
        *   **策略**：`[S85]` **每个时钟周期都切换线程**，轮流从不同的线程中取一条指令发射（Round-Robin）。
        *   **优点** `[S87]`：能极好地**掩盖短时间和长时间的流水线停顿**。由于线程切换频繁，一个线程的停顿完全不影响其他线程的执行。
        *   **缺点** `[S87]`：**单个线程的执行时间变长了**。一个本来可以连续执行的线程，现在被其他线程的指令“插队”，导致其完成时间被拖慢。
        *   **硬件** `[S86]`：需要为每个线程复制一套PC和寄存器文件。流水线的每个阶段都需要知道当前指令属于哪个线程。
    *   **粗粒度多线程 (Coarse-Grained Multithreading)** `[S88]`
        *   **策略**：`[S88]` 只有当当前线程遇到**长时间停顿事件**（如 Cache Miss）时，才切换到另一个线程。
        *   **优点** `[S89]`：单个线程在没有停顿时可以独占流水线，执行速度快，不会被拖慢。
        *   **缺点** `[S89]`：
            *   无法掩盖短时间的停顿。
            *   线程切换时有较大的性能开销，因为需要**冲刷 (Flush)** 流水线。
            *   可能存在公平性问题，一个不怎么停顿的线程会“霸占”处理器。

# Cache一致性


*   **两种处理写操作的方式** `[S94]`：
    1.  **更新协议 (Update Protocol)**：
        *   **操作**：`[S94]` 当处理器 A 写入数据 X 时，它会**广播**新的数据值。其他拥有 X 副本的处理器（如 B）会**更新**自己 Cache 中的值为新值。
        *   **优点** `[S97]`：如果共享数据被频繁地读写，更新可以减少后续读取时的 Miss 开销。
        *   **缺点** `[S97]`：如果一个数据被一个处理器连续多次写入，而其他处理器在此期间并未读取它，那么每次写入都广播新值就是一种带宽浪费。

    2.  **无效化协议 (Invalidate Protocol)**：
        *   **操作**：`[S94]` 当处理器 A 写入数据 X 时，它会广播一个**无效化消息**。其他拥有 X 副本的处理器 B 会将自己 Cache 中的副本标记为**无效 (Invalid)**。
        *   `[S97]` 这样，处理器 A 就获得了该数据块的**独占写入权**。
        *   下次处理器 B 需要读取 X 时，会发生 Cache Miss，然后从 A 或主存中获取最新的数据。
        *   **优点** `[S97]`：连续多次写入同一个块只需要一次无效化广播，节省了带宽。
        *   **缺点** `[S97]`：如果数据在处理器间频繁地交替读写，会导致“乒乓效应” (Ping-Pong Effect)：A 写导致 B 无效 -> B 读导致数据迁移 -> B 写导致 A 无效... 造成大量 Miss。
        *   **现代主流**：无效化协议是目前绝大多数商用多处理器系统的选择。

#### **知识卡片：消息传播机制 `[S95]`**

*   **要解决的问题**：
    上面说的“广播”和“通知”，在硬件上是如何实现的？

*   **两种主流机制** `[S95]`：
    1.  **侦听式 (Snoopy-based)**：
        *   **机制**：`[S95]` 所有处理器都连接到一个**共享总线 (Bus)** 上。每个处理器的 Cache 控制器都会“**侦听**”总线上发生的所有事务（如读、写请求）。
        *   **工作方式**：当一个处理器想进行读写时，它会先在总线上广播它的意图。其他处理器侦听到这个广播后，会检查自己的 Cache 状态，并做出相应的响应（如提供数据、或使自己的副本无效）。
        *   **顺序保证** `[S95]` `[S96]`：共享总线本身就是一个天然的**序列化设备**。所有请求都必须通过总线仲裁来获得使用权，这为所有内存操作提供了一个**全局统一的顺序 (Total Order)**，大大简化了一致性协议的设计。
        *   **适用性** `[S102]`：简单高效，但总线会成为瓶颈，因此**适合小规模的多处理器系统**。

    2.  **目录式 (Directory-based)**：
        *   **机制**：`[S95]` 系统中有一个**集中的目录 (Directory)**，它为内存中的**每一个 Block** 都维护一条记录。这条记录会追踪：**哪个处理器持有该 Block 的副本**，以及副本处于什么状态（是共享只读，还是独占可写）。
        *   **工作方式**：处理器不直接广播。当处理器 A 需要访问数据 X 时，它会向管理 X 的目录发送请求。目录根据记录，向持有 X 副本的其他处理器（如 B, C）发送点对点的消息（如“请把你的副本置为无效”或“请把你的数据发给A”）。
        *   **适用性**：避免了总线瓶颈，扩展性更好，**适合大规模的多处理器系统**，但设计更复杂，延迟可能更高。



#### **知识卡片：MSI 协议 `[S98]`**

*   **核心思想**：
    为每个 Cache Block 维护 3 个状态中的一个。

*   **三种状态** `[S98]`：
    *   **M (Modified, 修改)**：
        *   **含义**：这是**全局唯一的、最新的**副本。`[S98]`
        *   数据是“**脏**”的 (Dirty)，即与主存内容不一致。
        *   本地处理器可以**自由地读写**这个块，无需通知任何人。
    *   **S (Shared, 共享)**：
        *   **含义**：可能有**多个处理器**持有该块的副本，且所有副本以及主存中的内容都是一致的 (“**干净**”的，Clean)。
        *   `[S98]` 本地处理器只能**读取**这个块。
    *   **I (Invalid, 无效)**：
        *   **含义**：本地的这个 Cache Block 内容是过时的、无效的，不能使用。`[S98]`

*   **状态转换 (简化版)** `[S99]` (见 [Fig·S99-1] 表格)：
    *   **本地 Load Miss**: 处理器想读一个数据，但本地是 **I** 状态。它会在总线上广播一个**读请求 (BusRd)**。
        *   如果其他处理器有 **M** 副本，它会响应并把数据写回主存，然后两个处理器都变为 **S**。
        *   如果其他处理器有 **S** 副本或只有主存有，它会从主存/其他Cache获得数据，变为 **S**。
    *   **本地 Store (写)**:
        *   从 **S** -> **M**: 处理器想写一个共享块。它必须先在总线上广播一个**独占读请求 (BusRdX)**，告诉所有其他持有者“你们的副本都无效了！”。其他处理器收到后将自己的状态变为 **I**。该处理器成功获得独占权，变为 **M**，然后执行写入。
        *   从 **M** -> **M**: 本地可自由写入，无需广播。
    *   **侦听到 BusRdX**: 其他处理器想写这个块。无论本地是 **S** 还是 **M**，都必须放弃，变为 **I**。
    ![[Pasted image 20260109192144.png]]
*   **MSI 的问题** `[S100]`：
    一个**不必要的写总线事务**。考虑以下情况：
    1.  处理器 A 读取了数据 X，此时只有 A 有 X 的副本，状态为 **S**。
    2.  现在 A 想写入 X。根据 MSI 协议，它必须先广播一个 BusRdX 来使其他（不存在的）副本无效。
    3.  这个 BusRdX 是完全多余的，因为全局本来就只有一个副本。

#### **知识卡片：MESI 协议 (MSI的改进) `[S101]`**

*   **核心思想** `[S101]`：
    为了解决 MSI 的问题，增加一个新状态，用来标识“**我是全局唯一的副本，而且我是干净的**”。

*   **新增状态：E (Exclusive, 独占)** `[S101]`：
    *   **含义**：这是**全局唯一的、干净的**副本。`[S101]`
    *   **好处**：当处理器想写入一个处于 **E** 状态的块时，由于知道自己是唯一的持有者，它可以**悄悄地、无需任何总线广播**，直接将状态变为 **M** 并执行写入。这被称为“**静默升级 (Silent Upgrade)**”。

*   **如何进入 E 状态？** `[S101]`：
    当一个处理器发起读请求 (BusRd) 时，如果在总线上**没有其他处理器响应**说“我也有”，那么它就知道自己拿到了一个独占的副本，可以将状态设为 **E**。

*   **MESI 的问题** `[S104]`：
    `[S104]` **从 M 状态到 S 状态的转换效率低下**。
    1.  处理器 A 持有块 X，状态为 **M** (脏数据)。
    2.  处理器 B 发起读请求 BusRd。
    3.  A 侦听到后，必须：
        *   **(1) 把数据写回主存**，以保证主存的数据是新的。
        *   (2) 把数据通过总线发给 B。
        *   (3) A 和 B 的状态都变为 **S**。
    4.  `[S104]` 问题在于第 (1) 步。这次**写回主存的开销很大，而且可能是不必要的**，因为可能马上又有处理器（比如C，甚至B自己）要写入这个块，让主存的数据再次失效。


# 内存一致性


*   **要解决的问题（直觉）**：
    我们刚学过的 **Cache 一致性 (Coherence)** 保证了“对于**同一个**内存地址 A，所有处理器看到的值都是一致的”。但这还不够。**内存一致性 (Consistency)** 要解决一个更微妙的问题：“当处理器 P1 修改了地址 A，然后又修改了地址 B，那么其他处理器 P2 是先看到 A 的变化，还是先看到 B 的变化，还是看到的顺序不一定？” 这个**顺序**问题至关重要。

*   **类比 / 直觉**：
    你和朋友在网上协同编辑一份文档。
    *   **Coherence**：你把标题改成了“最终版”，朋友能看到这个新标题，而不是旧的“草稿版”。
    *   **Consistency**：你先改了标题，然后又在文末加了一句“全文完”。你朋友是先看到新标题再看到“全文完”，还是可能先看到“全文完”而标题还是旧的？如果硬件和编译器为了效率进行了“重排”，后一种情况就可能发生，导致逻辑混乱。

*   **形式化表述（严格）** `[S109]`：
    内存一致性模型是**硬件/编译器与程序员之间的一个约定 (Contract)**，它定义了在多处理器系统中，内存操作（读/写）的顺序应该是什么样的。硬件和编译器在进行性能优化（如指令重排）时，**不能违反这个约定**。

*   **为什么需要这个约定？** `[S109]`
    *   看这个例子：
        ```c
        // 初始值: A = 0, Flag = 0
        
        // 处理器 P1
        A = 23;
        Flag = 1;
        
        // 处理器 P2
        while (Flag != 1) { /* spin */ }
        print(A); // P2 期望打印出 23
        ```
    *   P1 的意图很明显：先准备好数据 `A`，然后用 `Flag` 通知 P2 数据已准备好。
    *   但如果硬件或编译器为了优化，将 P1 的两条写指令**重排**了，先执行 `Flag = 1`，再执行 `A = 23`。
    *   这时，P2 可能看到 `Flag` 变为1，跳出循环，然后去读取 `A`，结果读到了旧值 0！程序逻辑被破坏。
    *   `[S109]` **内存模型就是为了禁止这种会破坏程序逻辑的重排序**。

#### **知识卡片：顺序一致性 (Sequential Consistency, SC) `[S110]`**

*   **定义**：最严格、最符合程序员直觉的内存模型。
    `[S110]` 一个系统被称为顺序一致的，如果其任何执行结果，都等同于“**所有处理器的操作，以某种单一的全局顺序（global total order）交错执行，并且每个处理器内部的操作顺序，都遵循其程序本身指定的顺序**”。

*   **直觉** `[S111]`：
    `[S111]` 就像所有处理器都在共同操作一个**单一的、非并行的主存**。内存操作就像被一个全局的调度器一个个地串行执行。所有处理器看到的内存事件顺序都是完全一样的。

*   **优点** `[S111]`：
    *   **简单、直观**：完全符合程序员的编程直觉，非常容易理解和推理。

*   **缺点** `[S112]`：
    *   **性能极差**：为了实现这种严格的全局顺序，硬件上几乎**禁止了所有高级的性能优化**。比如，写缓冲、乱序执行、指令重排等技术，因为它们都可能破坏 SC 要求的严格顺序。
    *   **不必要的限制**：`[S112]` 很多时候程序并不需要这么强的顺序保证，SC 的限制过于严苛。


# GPU


#### **知识卡片：GPU 设计思路：从 CPU 到 GPU `[S117]`**

*   **CPU (Central Processing Unit)**：
    *   **设计目标**：**低延迟 (Low Latency)**。追求让**单个任务/线程**尽快完成。
    *   **手段**：复杂的控制逻辑、强大的ALU、乱序执行、分支预测、大容量Cache。核心是“**少而精**”。

*   **GPU (Graphics Processing Unit)**：
    *   **设计目标**：**高吞吐 (High Throughput)**。追求在单位时间内完成**尽可能多的任务/线程**。
    *   **核心思想转变**：放弃对单个线程延迟的极致追求，转而用海量的并行计算单元来处理海量的数据。

*   **GPU 的三大设计法宝**：
    1.  **简化流水线，增加核心数** `[S117]`
        *   **做法**：`[S117]` 对处理器核心进行“瘦身”，去掉昂贵的乱序执行、分支预测等复杂逻辑。
        *   **目的**：`[S118]` 把节省下来的晶体管和面积，用来制造**海量**的、简单的计算核心（ALU）。
    2.  **单指令多线程 (Single Instruction, Multiple Threads - SIMT)** `[S119]`
        *   **做法**：`[S119]` 让多个核心（一个执行单元组）共享同一个取指/译码单元。在同一个周期，这个组里的所有核心**执行同一条指令**，但处理的是**各自不同的数据**。
        *   **NVIDIA 术语** `[S121]`：
            *   **线程 (Thread)**：执行计算的最小单位。
            *   **Warp**: 一组（通常是32个）被捆绑在一起、共同执行指令的线程。是 GPU 硬件调度的基本单位。
            *   **SM (Streaming Multiprocessor)**：一个 GPU 核心，包含了SIMT执行单元、寄存器文件、共享内存等，可以调度和执行多个 Warps。
    3.  **同时驻留大量线程** `[S122]`
        *   **做法**：`[S122]` 在每个 SM 上都维护远多于其执行单元数量的 Warps 的上下文。
        *   **目的**：利用**细粒度的多线程**来**隐藏内存访问延迟**。当一个 Warp 因为等待内存数据而停顿时，SM 的调度器可以**立刻 (zero-overhead)** 切换到另一个已经就绪的 Warp 来执行，从而让计算单元始终保持繁忙。


另外就主要是那道作业题warp。





[^1]: 
	### 一、 页表结构与映射方式 (Mapping Mechanism)
	
	#### 1. 核心概念
	分页系统的核心思想是将**逻辑地址**（程序看到的）和**物理地址**（内存条实际的）解耦。
	*   **页 (Page)**：逻辑地址空间划分的块（由编译器/操作系统决定）。
	*   **页框 (Page Frame)**：物理内存划分的块（由硬件决定）。
	*   **大小相等**：页的大小必须等于页框的大小（例如都是 4KB）。
	
	#### 2. 地址结构
	一个逻辑地址分为两部分：
	$$逻辑地址 = 页号 (P) + 页内偏移量 (d)$$
	
	#### 3. 映射过程 (重点)
	CPU 拿到逻辑地址后，**MMU (内存管理单元)** 负责转换：
	1.  **提取页号 (P)**：根据逻辑地址的高位。
	2.  **查页表**：以 P 为索引，去页表中查找对应的**物理块号 (F)**。
	3.  **拼接物理地址**：用查到的物理块号 (F) 替换页号 (P)，**偏移量 (d) 保持不变**。
	    $$物理地址 = 物理块号 (F) \times 页大小 + 偏移量 (d)$$
	
	> **直观理解**：就像住酒店。
	> *   逻辑地址：你的房卡写着“第 5 层，03 号房”。
	> *   页表（前台记录）：第 5 层对应的实际是“B 座大楼”。
	> *   物理地址：你去“B 座大楼，03 号房”。(房间号即偏移量，是不变的)
	
	---
	
	### 二、 页表项 (PTE) 的含义
	
	页表不仅仅是一个简单的“数组”，每一个元素（页表项，Page Table Entry）包含关键的控制信息。
	
	*   **物理块号 (PFN)**：映射到的实际物理内存位置。
	*   **状态位 (Valid/Invalid)**：该页是否已调入内存？（0 表示在磁盘上，访问会触发缺页中断）。
	*   **修改位 (Dirty Bit)**：该页是否被写过？（如果被写过，换出时需要写回磁盘；否则直接丢弃）。
	*   **访问位 (Reference Bit)**：最近是否被访问过？（用于 LRU 等页面置换算法）。
	*   **读/写保护位**：该页是只读的还是可读写的？
	
	---
	
	### 三、 页表大小的含义与计算 (Calculation)
	
	#### 1. 为什么页表会很大？
	页表本身也是存储在内存中的数据。每个进程都有自己的页表。
	
	#### 2. 计算公式
	$$页表大小 = 页表项数 \times 每个页表项的大小$$
	$$页表项数 = \frac{逻辑地址空间大小}{页面大小}$$
	
	#### 3. 经典案例计算
	假设：系统为 **32位**，页面大小为 **4KB ($2^{12}$ B)**，页表项大小 **4 Byte**。
	*   **逻辑地址空间**：$2^{32}$ (4GB)。
	*   **页面数量**：$2^{32} / 2^{12} = 2^{20}$ (100万个页)。
	*   **页表大小**：$2^{20} \times 4B = 4MB$。
	
	**含义**：每个运行的进程（哪怕只写了一行 Hello World），系统都需要为此预留 **4MB** 的连续内存来存放它的页表。如果有 100 个进程，光页表就吃掉 400MB 内存。这就是为什么要引入多级页表的原因。
	
	---
	
	### 四、 增减页大小的影响 (Trade-offs)
	
	#### 1. 减小页面大小 (例如 4KB $\to$ 512B)
	*   **优点**：
	    *   **减少内部碎片 (Internal Fragmentation)**：文件的最后一页浪费的空间更少。
	    *   **内存利用率高**：按需分配更精确。
	*   **缺点**：
	    *   **页表急剧膨胀**：页面变小 $\to$ 页的数量变多 $\to$ 页表变大（占用更多内存）。
	    *   **I/O 效率降低**：与磁盘交换数据过于频繁（磁盘喜欢读写大块数据）。
	    *   **TLB 命中率下降**：TLB 容量有限，页越小，TLB 能覆盖的内存范围越小，导致频繁查内存页表，系统变慢。
	
	#### 2. 增大页面大小 (例如 4KB $\to$ 4MB "Huge Pages")
	*   **优点**：
	    *   **页表极小**：节省内存。
	    *   **TLB 命中率高**：一个 TLB 条目就能覆盖很大一片内存，CPU 访存速度快（数据库常用此优化）。
	    *   **I/O 效率高**：一次性加载大块数据。
	*   **缺点**：
	    *   **内部碎片严重**：哪怕只存 1KB 数据，也要占用 4MB 物理内存，浪费巨大。
	---
	

[^2]:   

	
	### 二、 Tomasulo 算法核心 (重点掌握)
	
	Tomasulo 算法是实现乱序执行的经典方案（IBM 360/91 引入），它巧妙地解决了数据相关性问题。
	
	#### 1. 保留站 (Reservation Station) 的作用
	
	保留站不仅仅是一个缓存，它**把“指令”和“计算单元”解耦**了。
	
	- **作用**：当指令发射到保留站时，如果操作数已经在寄存器里，就直接读进来；如果操作数还没算出来，保留站就记录下**“我在等谁”**（即记录产生该数据的保留站编号/Tag）。
	    
	- **意义**：指令一旦进入保留站，就不再依赖通用寄存器堆（Register File）了，而是在等待 CDB 广播的数据。
	    
	
	#### 2. 寄存器重命名 (Register Renaming) —— 消除相关性
	
	这是 Tomasulo 最精髓的地方。
	
	- **问题**：程序中经常复用同一个寄存器（如 R1），导致**假相关**（WAR：写后读，WAW：写后写）。
	    
	    - 例子：DIV F0, F2, F4 (算得慢) -> ADD F0, F6, F8 (算得快)。如果 ADD 先写回 F0，DIV 后写回，F0 的值就错了（WAW）。
	        
	- **解决**：
	    
	    - Tomasulo 算法中，**保留站的编号 (Tag)** 实际上取代了**寄存器号**。
	        
	    - 当指令发射时，如果目标寄存器是 F0，系统会标记：“现在 F0 的最新值将由 **保留站#3** 产生”。
	        
	    - 后续指令如果想读 F0，就会被告知：“去等 **保留站#3** 的结果”，而不是去读物理寄存器 F0。
	        
	- **结论**：通过重命名，**消除了 WAR 和 WAW 冒险**，让指令可以并行跑，只保留了真正的 RAW（数据依赖）相关。
	
	### 三、 精确异常与 ROB (Reorder Buffer)
	
	基本的 Tomasulo 算法有一个致命弱点：**它不支持精确异常**。因为指令是乱序写回的，如果中间某条指令出错了（如除以零），后面的指令可能已经修改了寄存器，现场无法完美恢复。
	
	#### 1. 精确异常 (Precise Exceptions)
	
	- **定义**：当发生中断或异常时，处理机状态必须精确地反映该指令之前的所有指令都已执行完，而该指令及其后的所有指令都未执行（或未修改状态）。
	    
	- **需求**：为了支持精确异常，我们不能让乱序执行的结果立刻修改寄存器堆。
	    
	
	#### 2. 重排序缓冲区 (Reorder Buffer, ROB)
	
	为了解决这个问题，在保留站和寄存器堆之间加了一层 **ROB**。
	
	- **工作原理**：
	    
	    1. **按序进入**：指令按程序顺序进入 ROB。
	        
	    2. **乱序执行**：指令在保留站里乱序计算。
	        
	    3. **结果暂存**：算完的结果先写进 ROB，**不直接写寄存器**。
	        
	    4. **按序提交**：只有当 ROB 队头的指令确认无异常且执行完毕，才将其结果写入真正的寄存器堆（Architectural Register File）。
	        
	
	---
	
	### 四、 关键对比：保留站 (RS) vs 重排序缓冲区 (ROB)
	
	这是考试中容易混淆的概念，请务必区分：
	
	|   |   |   |
	|---|---|---|
	|特性|保留站 (Reservation Station)|重排序缓冲区 (Reorder Buffer)|
	|**主要目的**|实现**乱序执行** (调度)|实现**按序提交** (保证正确性/精确异常)|
	|**存放内容**|待执行的指令 + **操作数的值** (或Tags)|已发射指令的**执行结果** + 状态位|
	|**核心作用**|**消除假相关** (重命名)，等待数据就绪|**缓冲结果**，确保发生异常时能回滚|
	|**数据流向**|此时数据还在“路上”（等待计算）|此时数据已经“算完”（等待官宣/退休）|
	|**何时清空**|一旦拿到操作数并**计算完成**，就释放 RS|只有指令**提交 (Commit)** 后，才释放 ROB|

[^3]: 
	### 一、 乱序执行 (Out-of-Order) 的核心逻辑
	
	传统的冯·诺依曼架构通常是“按序执行”的（一条堵住，后面全停）。乱序执行的核心目标是：**填满流水线，不让 CPU 空转。**
	
	#### 1. 核心思想
	
	- **按序发射 (In-order Issue)**：指令按程序写的顺序进入处理器。
	    
	- **乱序执行 (Out-of-order Execute)**：只要操作数准备好了，谁先就绪谁先跑，不用排队等待前面的慢指令。
	    
	- **按序提交 (In-order Commit)**：结果写回必须按原顺序，以保证程序逻辑正确（需配合 ROB，详见后文）。
	    
	
	#### 2. 所需硬件结构及作用
	
	为了实现上述目标，CPU 内部结构发生了变化：
	
	- **指令队列 (Instruction Queue)**：指令的“候诊区”，等待发射。
	    
	- **保留站 (Reservation Stations, RS)**：**这是核心**。分布在各个计算单元（如加法器、乘法器）前的缓冲区。它们暂存指令和操作数，等待数据就绪。
	    
	- **公共数据总线 (Common Data Bus, CDB)**：一条广播线路。计算结果算出来后，通过这里广播给所有在等待这个数据的保留站。
	    
	
